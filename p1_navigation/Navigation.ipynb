{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "*Jun Zhu, zhujun981661@gmail.com, 08.2020*\n",
    "\n",
    "In this notebook, I present the solution for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dqn_agent import DqnAgent\n",
    "from utilities import check_environment, play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brain_name, state_space, action_space = check_environment(env)\n",
    "brain_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the environment with random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play(env, brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train an agent with deep-Q network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm first proposed in this [nature paper](https://www.nature.com/articles/nature14236) was implemented. In addition, the following enhancements are included:\n",
    "\n",
    "1. [Dueling network architecture](https://arxiv.org/abs/1511.06581)\n",
    "2. [Double Q-learning](https://arxiv.org/abs/1509.06461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BananaBrainQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # state_space = 37\n",
    "        # action_space = 4\n",
    "        self._fc1 = nn.Linear(state_space, 256)\n",
    "        self._fc2 = nn.Linear(256, 256)\n",
    "        self._fc3 = nn.Linear(256, 64)\n",
    "                \n",
    "        # advantange for each action\n",
    "        self._fc_adv = nn.Linear(64, action_space)\n",
    "        # value which is independent of the action\n",
    "        self._fc_value = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for fc in (self._fc1, self._fc2, self._fc3):\n",
    "            x = F.relu(fc(x))\n",
    "            \n",
    "        v = self._fc_value(x)\n",
    "        a = self._fc_adv(x)\n",
    "        return v + (a - a.mean(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions = np.arange(action_space)\n",
    "target_score = 13\n",
    "\n",
    "# initialize the model\n",
    "model = BananaBrainQNetwork()\n",
    "\n",
    "# initialize the agent\n",
    "dqn_agent = DqnAgent(model, actions, \n",
    "                     replay_memory_size=100000,  # size of the experience replay buffer\n",
    "                     double_dqn=True  # use double Q-learning\n",
    "                    )\n",
    "\n",
    "# train the agent with given hyperparameters (remove ./dqn_checkpoint.pth to train a new model from scratch)\n",
    "scores = dqn_agent.train(env,\n",
    "                         n_episodes=2000,\n",
    "                         epsilon_decay_rate=0.995,  # decay rate of epsilon (1.0 - 0.01)\n",
    "                         target_network_update_frequency=4,  # frequency of updating target network\n",
    "                         gamma=0.99,  # discount factor\n",
    "                         learning_rate=5e-4,  # learning rate\n",
    "                         batch_size=64,  # mini batch size\n",
    "                         output_frequency=50,\n",
    "                         save_frequency=100,\n",
    "                         target_score=target_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learning history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "x = np.arange(len(scores)) + 1\n",
    "ax.plot(x, scores, alpha=180, label='score history')\n",
    "ax.plot(x[99:], np.convolve(scores, np.ones(100) / 100, 'valid'), label='score history (moving averaged)')\n",
    "ax.plot(x, target_score * np.ones_like(x), '--', label='reward (target)')\n",
    "ax.set_xlabel(\"Episode\", fontsize=16)\n",
    "ax.set_ylabel(\"Score\", fontsize=16)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Further improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithms could further improve the performance of the agent:\n",
    "    \n",
    "- [Prioritized experience replay](https://arxiv.org/abs/1511.05952)\n",
    "\n",
    "  It should be noted that there is performance concern over this algorithm.\n",
    " \n",
    "- [Distributional DQN](https://arxiv.org/abs/1707.06887)\n",
    "- [Noisy network](https://arxiv.org/abs/1706.10295)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}